# Mitigating Bias against People with Disabilities in Language Models

As the applications utilizing Natural Language Processing (NLP) technologies have increased steadily, it becomes crucial to recognize unintentional social bias it may impact and to build an equitable and inclusive Machine Learning model. However, not a few language models reflect social biases in training it with extensive real-world data.

In this project, we demonstrate the implicit disability bias of English language models in toxic classification. We review previous studies on detecting and mitigating social bias in language models and discuss the bias, especially towards mentions of disability. Next, we compare CNN-LSTM models with and without data augmentation or fine-tuning to quantify their effect on reducing disability bias. Lastly, we address future studies to expand our methods beyond the toxic classification.
